import os
from stable_baselines3 import DQN
from stable_baselines3.common.callbacks import CheckpointCallback
from stable_baselines3.common.vec_env import DummyVecEnv

from objects.snakeEnv import SnakeEnv


DEVICE="cpu"
# ============
# ENTRENAMIENTO
# ============
env = DummyVecEnv([lambda: SnakeEnv()])

# Carpeta donde se guardarÃ¡n los checkpoints
checkpoint_dir = "./checkpoints_snake/"
os.makedirs(checkpoint_dir, exist_ok=True)

# Callback para guardar cada 100k timesteps
checkpoint_callback = CheckpointCallback(
    save_freq=100000,  # cada 100k pasos
    save_path=checkpoint_dir,
    name_prefix="snake_dqn"
)

# Si ya existe un modelo, cargarlo; si no, crear uno nuevo
model_path = os.path.join(checkpoint_dir, "snake_dqn_latest.zip")
if os.path.exists(model_path):
    print("ðŸ”„ Cargando modelo entrenado...")
    model = DQN.load(model_path, env=env)
else:
    print("ðŸ†• Creando modelo nuevo...")
    model = DQN("MlpPolicy", env, verbose=1, device=DEVICE)

# ENTRENAR
model.learn(
    total_timesteps=1000000,  # puedes subirlo a millones si quieres
    callback=checkpoint_callback
)

# Guardar el Ãºltimo estado entrenado
model.save(model_path)
print("âœ… Entrenamiento terminado y modelo guardado.")